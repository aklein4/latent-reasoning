
model_type: base
architectures: [
    BaseLmModel
]

max_sequence_length: 16

hidden_size: 128
mlp_size: 256

num_layers: 4
num_attention_heads: 2

use_qkv_bias: true
hidden_act: silu
layer_norm_eps: 0.00001

use_rope: true
rope_fraction: 2
rope_base: 10000

initializer_range: 0.02
identity_init: false

ignore_segment_ids: false
gradient_checkpointing: false

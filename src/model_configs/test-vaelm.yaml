
model_type: vaelm
architectures: [
    VaeLmModel
]

max_sequence_length: 16

hidden_size: 128
mlp_size: 256

attention_head_size: 32
num_attention_heads: 2
num_registers: 7

num_layers: 4

hidden_act: silu
layer_norm_eps: 0.00001

use_rope: true
rope_fraction: 2
rope_base: 10000

thought_length: 13
z_size: 44

gradient_checkpointing: false
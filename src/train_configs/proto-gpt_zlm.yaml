trainer_type: zlm

dataset: GPT2-fineweb-100BT-split-128
collator_type: split
collator_kwargs: {}

bs: 512
num_epochs: 1

optimizer_type: adamw
optimizer_kwargs:

    num_warmup_steps: 100
    num_training_steps: 30000

    lr: 0.0003
    final_lr: 0.00003

    betas: [0.9, 0.95]
    eps: 0.00000001
    weight_decay: 0.01

checkpoint_interval: 2500
save_optimizer: false

info_threshold: 1.0
info_total: 1000

kl_weight: 0.1

acc_threshold: 0.99

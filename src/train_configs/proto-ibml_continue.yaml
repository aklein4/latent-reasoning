trainer_type: ibml

dataset:
    name: fineweb-edu-TinyLlama
    branch: split-2

    type: masked
    kwargs:
        sequence_length: 1024
        pad_token_id: 0

bs: 64
num_epochs: 1

optimizer_type: adamw
optimizer_kwargs:

    num_warmup_steps: 1000
    num_training_steps: 500000000000.0

    lr: 0.00000
    final_lr: 0.000000

    betas: [0.9, 0.95]
    eps: 0.01
    weight_decay: 0.01

    grad_clip: 3.0

checkpoint_interval: 999999
save_optimizer: false

init_hooked: true
init_hooked_steps: 999999

hook_acc: 0.75

min_help_scale: 0.01
help_steps: 10000

max_beta: avg
beta_steps: 50000

val_interval: 1
